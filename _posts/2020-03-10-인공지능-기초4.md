---
date: 2020-03-10 24:00:00
layout: post
title: 인공지능-기초4
subtitle: 기초이론 공부
description: 인공지능에 대한 기초이론을 공부를 해보자
image: /assets/img/work/프로젝트1.jpg
optimized_image: /assets/img/work/프로젝트2.jpg
category: project
tags:
  - 인공지능
  - 학습
  - 기초
  - 데이터마이닝
author: 김대훈
---

## 프로젝트 소개

인공지능 기초이론 학습

## 인공지능 기초이론 4

> Deep Natural Language Processing(NLP)

단어, 문장들 속에서 의미를 찾아내는 것이 NLP의 궁극적인 목표

- Word, Sentence, and Document Embedding

- Word Vector Representations : "Word Embedding"

> 단어를 벡터로 표현하는 언어 모델링 방법

- Continuous(dense) Vector Representations

  - 하나의 단어가 실수 여러개로 표현

- One-hot Vector Representations
  - 다차원의 공간(디멘션)에서 비슷한 단어는 그래프 내에서 가까운 공간에 위치한다.
    벡터로 표현하는 이유라고 할 수 있다

> Distributed Representations

하나의 컨셉이 여러개의 뉴런으로 표현되고, 하나의 뉴런이 여러개의 컨셉으로 표현 가능하다.

- Many-to-many relationship

  > - a concept - many neurons
  > - a neuron - many concepts

- Distributed Representations of Words
  > ==Word Vector들을 학습한 그래프==<br/>
  >
  > - 단어들의 의미적인, 문법적인 특성이 반영된다

> Distributional Hypothesis

같은 문맥에서 나오는 단어는 유사한 의미를 갖고 있을 것이다.

- 단어를 벡터로 표현하고, Distributed Representations하는 것의 유래

> Word2Vec (NIPS 2012, ICLR 2013)

CBOW, Skip-Gram 2가지 버젼이 있다.

- Word를 Vector로 학습
- 둘 다 안보이는 단어를 예측
- CBOW
  - "문맥에 있는 단어들이 주어지고 중간에 있는 단어가 무엇이냐?"
- Skip-Gram
  - "중간의 단어를 아는 상태에서 주변에 나올 수 있는 단어가 무엇이냐?"

> Evaluation : Word Similarity Task

학습한 Word Vector들을 어떻게 평가할 것인가?

- 두가지 방법

  1.  Word Similarity Task

      - 사람들이 어떠한 단어를 얼마나 유사하다고 생각한 값, Cosine similarity으로 나온 값을 비교

      $$
       cos(a,b) = \frac {a\cdot b} {||A||*||b||}
      $$

  2.  Word Analogy Task
      - 어떤 단어가 같은 방향성, 같은 거리로 갔을 때 ==비슷한 관계를 얼마나 잘 찾아내느냐?

> Problems of word-level approach

- Out-of-vocabularies : Unseen words
  - Morphologically rich languages
    - "예쁜,예쁘다, 예쁨, 예뻤다" 같이 ==같은 뜻이지만 다른 input
  - Compositionality of words
    - "미인, 예쁜 사람" 같이 서로 다른 뜻의 단어들을 조합하여 같은 의미가 되더라도, 같은 의미로 받아들일 수 없다=
- Quality of vectors assigned to rare words
  - 테스트 중 굉장히 적은 횟수로 등장한 (의미없는) 단어들
- Segmentation issues

> FastText ( =Subword Information Skip-Gram)

다른 유닛에 대해서도 Embedding을 진행하는 방식

> NLP Trend : Transfer Learning from Lanuage Models

1. Elmo

- Word Embedding의 문제 중 ++하나가 Context가 없었다.
- Contextualized Word Embedding
- **Context에 따라 Embedding이 달라진다**
- ==그 단어가 어떤 Context에서 나왔는 지 고려한 Model
- 문맥에 의존하는 방법 중 단방향으로 의존한다.

2. Bert

- 긴 시퀀스에 대해서 잘 학습을 하지못하는 문제
  - Transformers로 해결
- 문맥에 의존하는 표현인데 단방향이 아닌 "양방향으로 의존을 하자"
- 기본적인 Word Embedding에서 "Position Embedding" 추가
- Task example
  1.  Sentence Pair Classification Tasks
  2.  Single Sentence Classification Tasks
  3.  Question Answering Tasks
  4.  Single Sentence Tagging Tasks
