---
date: 2020-03-09 24:00:00
layout: post
title: 인공지능-기초2
subtitle: 기초이론 공부
description: 인공지능에 대한 기초이론을 공부를 해보자
image: https://res.cloudinary.com/dm7h7e8xj/image/upload/v1559822137/theme11_vei7iw.jpg
optimized_image: https://res.cloudinary.com/dm7h7e8xj/image/upload/c_scale,w_380/v1559822137/theme11_vei7iw.jpg
category: project
tags:
  - 인공지능
  - 학습
  - 기초
  - 데이터마이닝
author: 김대훈
---

## 🎤 프로젝트 소개

인공지능 기초이론 학습

## 인공지능 기초이론

# Linear Regression

- 지도학습
  사실 지도학습 중 많은 부분은 분류 말고도 Regression 형태로 함
  주어진 x에 대해 y값을 실수로 예측 하는 것
  classification : 데이터가 주어졌을 때 자동차냐 아니냐 (binary classification),
  자동차냐 사람이냐 고양이냐 처럼 (multi class classification)
  Regression: 데이터가 주어졌을 때 원하는 결과값이 실수(3.5나 6.2처럼)임

ex) 집의 평수에 따라 가격이 나오는 것, customer의 feature에 따라 매장에서 돈을 얼마나 쓸 것인가

> 그래프
> 주어진 데이터에 대해서는 y값을 다 알고있음
> 선을 그리는 이유 : x값이 14.5일 때, y값이 무엇일지 현재는 알 수 없지만, line의 값을 통해 알 수 있음
> line은 사실 다르게 그릴 수 있음
> 하지만, 데이터에 잘 맞지 않게 그리는 line도 있을 수 있음
> 수학적으로 가장 적합한 라인을 그릴 수 있음

- y = ax+b로 선을 나타낼 수 있는데, 이것을 y = w1x + w0 처럼 씀 (convention임)

- response (real number) is a linear function of the inputs
  y(x) = w^T \* x + E(epsilon표기임)
  가장 적합한 w를 찾아내자!

> Modeling non-linear relationships

w^T \* x 대신에, w^T ø(x)를 사용하여, 파이라는 비선형적인 function을 넣어줌
ex) ø(x) = x^2 또는 ø(x) = 2x+3x^2 처럼 사용될 수 있음

> Polynomial Regression

x, x^2, x^14, x^20.. 더 복잡한 function이 등장함
14나 20정도는 정확한 값을 얻기는 힘들 것
하지만, x보다는 x^2이 더 잘 설명해 줄 수 있음
그래서 linear regression을 하면서 polynomial function을 첨가하는 것임

> Multivariate linear regression

variable이 x1,x2의 조합으로 y를 예측할 수 있다
w0 + w1x1+ w2x2

> Maximum likelihood estimation

y = w1x + w0에서 w를 어떻게 잘 찾아낼 것이냐

> Residual Sum of Squares

적합한 라인을 그리는 w라는 것은, RSS임
앞서, 좌표평면에 y=ax+b 형태가 y=a(a는 상수) 형태보다 좋다고 판단
그 이유는, 실제 y값과 예측된 값의 차이가 y=ax+b가 더 적기 때문
to minimize NLL, we minimize this term,
sigma(i=1 to N) (yi - w^Txi)^2 called RSS! (실제와 예측의 y차이를 제곱한것의 합)

> Least Squres

MLE for w는 RSS를 minimize한 것임
(RSS를 그림으로 나타낸 그래프에서)

> Ridge Regression

linear라 하더라도 비선형 function이 있을 수 있었음
w를 찾자면, 찾은 선과 실제 데이터와 가까운 선을 찾았기 때문에 복잡했었음
MLE can overfit (쓸데없이 복잡한 curve가 나온 경우)
말이 안되게 나오는 경우도 있음
그래서, Regularization : 복잡하지 않고, 심플하게 만들자!
w와 같은 계수들이 값으로 나오는데, 크기가 커서 좋지 않음
그래서 크기가 큰 값에 대해 panelty를 주자
RSS에서 수식이 sigma(i=1 to N) (yi - w^Txi)^2 였다면,
여기에 개념적으로 추가된 것은 + λ||w||\_2^2
w들에 대해 +를 해서 제곱해서 더함 -> panelty로 작용
w들의 값 자체를 작게 만드는 w를 찾자는 것
결과 그래프를 보면, 이전보다 simple해짐

> Regularization effects of big data

w^의 sum은 l_2norm 혹은 l_2regularization이라고 함
또다른 Regularization 방법은 데이터의 양을 늘리는 것
모델이 자연스럽게 Regularization 을 하기 때문임

# 그래프 1>2>3>4

1st graph) degree=1, truth degree=2 (실제 데이터를 만들어낸 모델)
red line = test data에 대한 error값
blue line은 training data에 대한 error값
black error = training 및 test data에 대한 이상적인 error값
적어도 training data에 대해서는 학습효과가 있어야 함
x^2인데 x만으로 fitting하려고 하니까 전혀 되고 있지 않음
2nd graph) degree=2, truth degree=2
test data 에 대한 error가 이상적인 값에 가고 있음
training data는 이상적인 값보다 낮은 곳에서 출발 = overfitting
미래 예측해야 하는 test data에 대해서는 error가 큼
x축은 data set의 크기임, x가 커질 수록 red line과 blue line이 같아짐 (좋은 모델)
3rd graph) degree=10, truth degree=2
complex한 모델의 경우임
training data에 대한 error는 매우 적게 시작해서 overfitting
overfit : training data에 대한 error와 test data에 대한 error의 차이가 얼마나 나는가
4th graph) degree=25
불필요하게 complex한 모델을 사용함에도 data set이 커질 수록, 괜찮은 모델로 평가됨
