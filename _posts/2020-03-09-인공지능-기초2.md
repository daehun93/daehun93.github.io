---
date: 2020-03-09 24:00:00
layout: post
title: ì¸ê³µì§€ëŠ¥-ê¸°ì´ˆ2
subtitle: ê¸°ì´ˆì´ë¡  ê³µë¶€
description: ì¸ê³µì§€ëŠ¥ì— ëŒ€í•œ ê¸°ì´ˆì´ë¡ ì„ ê³µë¶€ë¥¼ í•´ë³´ì
image: https://res.cloudinary.com/dm7h7e8xj/image/upload/v1559822137/theme11_vei7iw.jpg
optimized_image: https://res.cloudinary.com/dm7h7e8xj/image/upload/c_scale,w_380/v1559822137/theme11_vei7iw.jpg
category: project
tags:
  - ì¸ê³µì§€ëŠ¥
  - í•™ìŠµ
  - ê¸°ì´ˆ
  - ë°ì´í„°ë§ˆì´ë‹
author: ê¹€ëŒ€í›ˆ
---

## ğŸ¤ í”„ë¡œì íŠ¸ ì†Œê°œ

ì¸ê³µì§€ëŠ¥ ê¸°ì´ˆì´ë¡  í•™ìŠµ

## ì¸ê³µì§€ëŠ¥ ê¸°ì´ˆì´ë¡ 

# Linear Regression

- ì§€ë„í•™ìŠµ
  ì‚¬ì‹¤ ì§€ë„í•™ìŠµ ì¤‘ ë§ì€ ë¶€ë¶„ì€ ë¶„ë¥˜ ë§ê³ ë„ Regression í˜•íƒœë¡œ í•¨
  ì£¼ì–´ì§„ xì— ëŒ€í•´ yê°’ì„ ì‹¤ìˆ˜ë¡œ ì˜ˆì¸¡ í•˜ëŠ” ê²ƒ
  classification : ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ìë™ì°¨ëƒ ì•„ë‹ˆëƒ (binary classification),
  ìë™ì°¨ëƒ ì‚¬ëŒì´ëƒ ê³ ì–‘ì´ëƒ ì²˜ëŸ¼ (multi class classification)
  Regression: ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì›í•˜ëŠ” ê²°ê³¼ê°’ì´ ì‹¤ìˆ˜(3.5ë‚˜ 6.2ì²˜ëŸ¼)ì„

ex) ì§‘ì˜ í‰ìˆ˜ì— ë”°ë¼ ê°€ê²©ì´ ë‚˜ì˜¤ëŠ” ê²ƒ, customerì˜ featureì— ë”°ë¼ ë§¤ì¥ì—ì„œ ëˆì„ ì–¼ë§ˆë‚˜ ì“¸ ê²ƒì¸ê°€

> ê·¸ë˜í”„
> ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” yê°’ì„ ë‹¤ ì•Œê³ ìˆìŒ
> ì„ ì„ ê·¸ë¦¬ëŠ” ì´ìœ  : xê°’ì´ 14.5ì¼ ë•Œ, yê°’ì´ ë¬´ì—‡ì¼ì§€ í˜„ì¬ëŠ” ì•Œ ìˆ˜ ì—†ì§€ë§Œ, lineì˜ ê°’ì„ í†µí•´ ì•Œ ìˆ˜ ìˆìŒ
> lineì€ ì‚¬ì‹¤ ë‹¤ë¥´ê²Œ ê·¸ë¦´ ìˆ˜ ìˆìŒ
> í•˜ì§€ë§Œ, ë°ì´í„°ì— ì˜ ë§ì§€ ì•Šê²Œ ê·¸ë¦¬ëŠ” lineë„ ìˆì„ ìˆ˜ ìˆìŒ
> ìˆ˜í•™ì ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ë¼ì¸ì„ ê·¸ë¦´ ìˆ˜ ìˆìŒ

- y = ax+bë¡œ ì„ ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ”ë°, ì´ê²ƒì„ y = w1x + w0 ì²˜ëŸ¼ ì”€ (conventionì„)

- response (real number) is a linear function of the inputs
  y(x) = w^T \* x + E(epsiloní‘œê¸°ì„)
  ê°€ì¥ ì í•©í•œ wë¥¼ ì°¾ì•„ë‚´ì!

> Modeling non-linear relationships

w^T \* x ëŒ€ì‹ ì—, w^T Ã¸(x)ë¥¼ ì‚¬ìš©í•˜ì—¬, íŒŒì´ë¼ëŠ” ë¹„ì„ í˜•ì ì¸ functionì„ ë„£ì–´ì¤Œ
ex) Ã¸(x) = x^2 ë˜ëŠ” Ã¸(x) = 2x+3x^2 ì²˜ëŸ¼ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ

> Polynomial Regression

x, x^2, x^14, x^20.. ë” ë³µì¡í•œ functionì´ ë“±ì¥í•¨
14ë‚˜ 20ì •ë„ëŠ” ì •í™•í•œ ê°’ì„ ì–»ê¸°ëŠ” í˜ë“¤ ê²ƒ
í•˜ì§€ë§Œ, xë³´ë‹¤ëŠ” x^2ì´ ë” ì˜ ì„¤ëª…í•´ ì¤„ ìˆ˜ ìˆìŒ
ê·¸ë˜ì„œ linear regressionì„ í•˜ë©´ì„œ polynomial functionì„ ì²¨ê°€í•˜ëŠ” ê²ƒì„

> Multivariate linear regression

variableì´ x1,x2ì˜ ì¡°í•©ìœ¼ë¡œ yë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤
w0 + w1x1+ w2x2

> Maximum likelihood estimation

y = w1x + w0ì—ì„œ wë¥¼ ì–´ë–»ê²Œ ì˜ ì°¾ì•„ë‚¼ ê²ƒì´ëƒ

> Residual Sum of Squares

ì í•©í•œ ë¼ì¸ì„ ê·¸ë¦¬ëŠ” wë¼ëŠ” ê²ƒì€, RSSì„
ì•ì„œ, ì¢Œí‘œí‰ë©´ì— y=ax+b í˜•íƒœê°€ y=a(aëŠ” ìƒìˆ˜) í˜•íƒœë³´ë‹¤ ì¢‹ë‹¤ê³  íŒë‹¨
ê·¸ ì´ìœ ëŠ”, ì‹¤ì œ yê°’ê³¼ ì˜ˆì¸¡ëœ ê°’ì˜ ì°¨ì´ê°€ y=ax+bê°€ ë” ì ê¸° ë•Œë¬¸
to minimize NLL, we minimize this term,
sigma(i=1 to N) (yi - w^Txi)^2 called RSS! (ì‹¤ì œì™€ ì˜ˆì¸¡ì˜ yì°¨ì´ë¥¼ ì œê³±í•œê²ƒì˜ í•©)

> Least Squres

MLE for wëŠ” RSSë¥¼ minimizeí•œ ê²ƒì„
(RSSë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ì—ì„œ)

> Ridge Regression

linearë¼ í•˜ë”ë¼ë„ ë¹„ì„ í˜• functionì´ ìˆì„ ìˆ˜ ìˆì—ˆìŒ
wë¥¼ ì°¾ìë©´, ì°¾ì€ ì„ ê³¼ ì‹¤ì œ ë°ì´í„°ì™€ ê°€ê¹Œìš´ ì„ ì„ ì°¾ì•˜ê¸° ë•Œë¬¸ì— ë³µì¡í–ˆì—ˆìŒ
MLE can overfit (ì“¸ë°ì—†ì´ ë³µì¡í•œ curveê°€ ë‚˜ì˜¨ ê²½ìš°)
ë§ì´ ì•ˆë˜ê²Œ ë‚˜ì˜¤ëŠ” ê²½ìš°ë„ ìˆìŒ
ê·¸ë˜ì„œ, Regularization : ë³µì¡í•˜ì§€ ì•Šê³ , ì‹¬í”Œí•˜ê²Œ ë§Œë“¤ì!
wì™€ ê°™ì€ ê³„ìˆ˜ë“¤ì´ ê°’ìœ¼ë¡œ ë‚˜ì˜¤ëŠ”ë°, í¬ê¸°ê°€ ì»¤ì„œ ì¢‹ì§€ ì•ŠìŒ
ê·¸ë˜ì„œ í¬ê¸°ê°€ í° ê°’ì— ëŒ€í•´ paneltyë¥¼ ì£¼ì
RSSì—ì„œ ìˆ˜ì‹ì´ sigma(i=1 to N) (yi - w^Txi)^2 ì˜€ë‹¤ë©´,
ì—¬ê¸°ì— ê°œë…ì ìœ¼ë¡œ ì¶”ê°€ëœ ê²ƒì€ + Î»||w||\_2^2
wë“¤ì— ëŒ€í•´ +ë¥¼ í•´ì„œ ì œê³±í•´ì„œ ë”í•¨ -> paneltyë¡œ ì‘ìš©
wë“¤ì˜ ê°’ ìì²´ë¥¼ ì‘ê²Œ ë§Œë“œëŠ” wë¥¼ ì°¾ìëŠ” ê²ƒ
ê²°ê³¼ ê·¸ë˜í”„ë¥¼ ë³´ë©´, ì´ì „ë³´ë‹¤ simpleí•´ì§

> Regularization effects of big data

w^ì˜ sumì€ l_2norm í˜¹ì€ l_2regularizationì´ë¼ê³  í•¨
ë˜ë‹¤ë¥¸ Regularization ë°©ë²•ì€ ë°ì´í„°ì˜ ì–‘ì„ ëŠ˜ë¦¬ëŠ” ê²ƒ
ëª¨ë¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ Regularization ì„ í•˜ê¸° ë•Œë¬¸ì„

# ê·¸ë˜í”„ 1>2>3>4

1st graph) degree=1, truth degree=2 (ì‹¤ì œ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¸ ëª¨ë¸)
red line = test dataì— ëŒ€í•œ errorê°’
blue lineì€ training dataì— ëŒ€í•œ errorê°’
black error = training ë° test dataì— ëŒ€í•œ ì´ìƒì ì¸ errorê°’
ì ì–´ë„ training dataì— ëŒ€í•´ì„œëŠ” í•™ìŠµíš¨ê³¼ê°€ ìˆì–´ì•¼ í•¨
x^2ì¸ë° xë§Œìœ¼ë¡œ fittingí•˜ë ¤ê³  í•˜ë‹ˆê¹Œ ì „í˜€ ë˜ê³  ìˆì§€ ì•ŠìŒ
2nd graph) degree=2, truth degree=2
test data ì— ëŒ€í•œ errorê°€ ì´ìƒì ì¸ ê°’ì— ê°€ê³  ìˆìŒ
training dataëŠ” ì´ìƒì ì¸ ê°’ë³´ë‹¤ ë‚®ì€ ê³³ì—ì„œ ì¶œë°œ = overfitting
ë¯¸ë˜ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” test dataì— ëŒ€í•´ì„œëŠ” errorê°€ í¼
xì¶•ì€ data setì˜ í¬ê¸°ì„, xê°€ ì»¤ì§ˆ ìˆ˜ë¡ red lineê³¼ blue lineì´ ê°™ì•„ì§ (ì¢‹ì€ ëª¨ë¸)
3rd graph) degree=10, truth degree=2
complexí•œ ëª¨ë¸ì˜ ê²½ìš°ì„
training dataì— ëŒ€í•œ errorëŠ” ë§¤ìš° ì ê²Œ ì‹œì‘í•´ì„œ overfitting
overfit : training dataì— ëŒ€í•œ errorì™€ test dataì— ëŒ€í•œ errorì˜ ì°¨ì´ê°€ ì–¼ë§ˆë‚˜ ë‚˜ëŠ”ê°€
4th graph) degree=25
ë¶ˆí•„ìš”í•˜ê²Œ complexí•œ ëª¨ë¸ì„ ì‚¬ìš©í•¨ì—ë„ data setì´ ì»¤ì§ˆ ìˆ˜ë¡, ê´œì°®ì€ ëª¨ë¸ë¡œ í‰ê°€ë¨
